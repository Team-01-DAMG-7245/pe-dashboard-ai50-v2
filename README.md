## Case Study 2 ‚Äî Project ORBIT (Part 2)  
### Agentification and Secure Scaling of PE Intelligence using MCP

---

## üß≠ Setting

In Assignment 4 (Project ORBIT Part 1) you automated ingestion and Markdown dashboard generation for the **Forbes AI 50** using Airflow ETL + FastAPI + Streamlit.  
That system worked, but it was **static**‚Äîno reasoning, no secure integration with multiple data tools.

Now, **Priya Rao (VP of Data Engineering)** wants you to evolve it into an **agentic, production-ready platform** that can:

- Orchestrate due-diligence workflows through **supervisory LLM agents**  
- Standardize tool access with the **Model Context Protocol (MCP)**  
- Employ **ReAct reasoning** for transparency  
- Run under **Airflow orchestration** with containerized MCP services  
- Pause for **Human-in-the-Loop (HITL)** review when risks appear  

---

## üéØ Learning Outcomes

By the end you will:

- Build specialized LLM agents (LangChain v1 or Microsoft Agent Framework)  
- Design a **Supervisory Agent Architecture** that delegates to sub-agents  
- Implement an **MCP server** exposing Tools / Prompts / Resources  
- Apply the **ReAct pattern** (Thought ‚Üí Action ‚Üí Observation) with structured logs  
- Compose a **graph-based workflow** (LangGraph or WorkflowBuilder) with conditional edges  
- Integrate **Airflow DAGs**, **Docker**, and **.env configuration** for deployment  
- Add **pytest tests** and structured logging for maintainability  
- Embed **Human-in-the-Loop (HITL)** approval nodes for risk verification  

---

## Architecture Diagram

The following diagram illustrates the complete system architecture for Assignment 4 - Project ORBIT Part 1, showing the two parallel dashboard generation pipelines (RAG and Structured), Airflow orchestration, and the complete data flow from ingestion to evaluation.

![Assignment 4 Architecture Diagram](docs/assignment4_architecture_diagram.png)

### Diagram Components

- **‚òÅÔ∏è Airflow Orchestration Layer**: 2 DAGs (Initial Load @once, Daily Refresh 0 3 * * *)
- **üì• Data Ingestion Layer**: Web Scraper (homepage, /about, /product, /careers, /blog) ‚Üí Raw Storage (S3/GCS)
- **üîÑ Processing Layer - Two Parallel Pipelines**:
  - **RAG Pipeline (Unstructured)**: Raw ‚Üí Chunk ‚Üí Embed ‚Üí Vector DB ‚Üí LLM ‚Üí Dashboard
  - **Structured Pipeline (Pydantic)**: Raw ‚Üí Instructor ‚Üí Pydantic Models ‚Üí Payload ‚Üí LLM ‚Üí Dashboard
- **üåê API & UI Layer**: FastAPI (port 8000) + Streamlit (port 8501)
- **üìä Evaluation & Comparison**: Rubric-based comparison of RAG vs Structured dashboards
- **üíæ Storage Layer**: S3/GCS, Vector DB (ChromaDB), Local storage

### Pipeline Flows

**RAG Pipeline:**
```
Raw HTML ‚Üí Text Chunker ‚Üí Vector DB (Embeddings) ‚Üí LLM (Top-K chunks) ‚Üí RAG Dashboard
```

**Structured Pipeline:**
```
Raw HTML ‚Üí Instructor (Pydantic Extraction) ‚Üí Structured Data ‚Üí Payload Assembly ‚Üí LLM (Structured context) ‚Üí Structured Dashboard
```

**To regenerate the diagram:**
```powershell
python scripts/generate_assignment4_architecture_diagram.py
```

---

## Submission Deliverables
1. **GitHub repo** : https://github.com/Team-01-DAMG-7245/pe-dashboard-ai50
2. **EVAL.md** : https://github.com/Team-01-DAMG-7245/pe-dashboard-ai50/blob/swara/EVAL.md
3. **Demo video** : https://drive.google.com/file/d/1abFljwrx1lSxF5LOVPup7tTnjKDvDKF8/view?usp=sharing
4. **Youtube video** : https://youtu.be/BzeN0LC2-8Q
5. **Reflection.md for Evaluation** : https://github.com/Team-01-DAMG-7245/pe-dashboard-ai50/blob/main/REFLECTION.md
## Quick Start

### Run Airflow (Docker)
```bash
docker compose up
# Access UI: http://localhost:8080 (admin/admin)
```

### Run App Locally (Dev)
```bash
python -m venv airflow_env
source airflow_env/bin/activate
pip install -r requirements.txt
uvicorn src.api:app --reload        # http://localhost:8000
streamlit run src/streamlit_app.py  # http://localhost:8501
```

---

## Project Structure

```
‚îú‚îÄ‚îÄ dags/                  # Airflow DAGs (Labs 2-3)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ forbes_ai50_seed.json   # Company list (Lab 0)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # Scraped HTML/text (Lab 1)
‚îÇ   ‚îú‚îÄ‚îÄ structured/              # Pydantic models (Lab 5)
‚îÇ   ‚îú‚îÄ‚îÄ payloads/                # Dashboard payloads (Lab 6)
‚îÇ   ‚îî‚îÄ‚îÄ workflow_dashboards/     # Lab 17 workflow outputs
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api.py                   # FastAPI endpoints (Lab 7-8)
‚îÇ   ‚îú‚îÄ‚îÄ models.py                # Pydantic schemas (Lab 5)
‚îÇ   ‚îú‚îÄ‚îÄ s3_utils.py              # Cloud storage (Lab 1)
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py         # Dashboard UI (Lab 10)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

---

## Setup

### 1. AWS S3 Configuration
```bash
aws configure  # Enter your credentials
export AWS_BUCKET_NAME=quanta-ai50-data
```

### 2. Seed File
Populate `data/forbes_ai50_seed.json` with Forbes AI 50 companies from https://www.forbes.com/lists/ai50/

### 3. Environment Variables
Create `.env`:
```bash
AWS_BUCKET_NAME=quanta-ai50-data
OPENAI_API_KEY=your-api-key-here
```

---

## üß± Project Architecture Overview

```mermaid
flowchart TD
    subgraph Airflow
        DAG1[Initial Load DAG]
        DAG2[Daily Update DAG]
        DAG3[Agentic Dashboard DAG]
    end
    subgraph Services
        MCP[MCP Server]
        AGENT[Supervisor Agent]
    end
    DAG3 -->|HTTP/CLI| MCP
    MCP --> AGENT
    AGENT -->|calls Tools| MCP
    AGENT -->|Risk Detected| HITL[Human Approval]
    AGENT --> STORE[(Dashboards DB or S3)]
```


üß© Phase 1 ‚Äì Agent Infrastructure & Tool Definition (Labs 12‚Äì13)

Lab 12 ‚Äî Core Agent Tools

Implement async Python tools with Pydantic models for structured I/O:

Tool	Purpose
get_latest_structured_payload(company_id)	Return the latest assembled payload from Assignment 2
rag_search_company(company_id, query)	Query the Vector DB for contextual snippets
report_layoff_signal(signal_data)	Log or flag high-risk events (layoffs / breaches)

‚úÖ Checkpoint: Unit tests (tests/test_tools.py) validate each tool‚Äôs behavior.

‚∏ª

Lab 13 ‚Äî Supervisor Agent Bootstrap
	‚Ä¢	Instantiate a Due Diligence Supervisor Agent with system prompt:
‚ÄúYou are a PE Due Diligence Supervisor Agent. Use tools to retrieve payloads, run RAG queries, log risks, and generate PE dashboards.‚Äù
	‚Ä¢	Register the three tools.
	‚Ä¢	Verify tool invocation loop via ReAct logs.

‚úÖ Checkpoint: Console logs show Thought ‚Üí Action ‚Üí Observation sequence.

‚∏ª

üåê Phase 2 ‚Äì Model Context Protocol (MCP) Integration (Labs 14‚Äì15)

Lab 14 ‚Äî MCP Server Implementation

Create src/server/mcp_server.py exposing HTTP endpoints:

Type	Endpoint	Description
Tool	/tool/generate_structured_dashboard	Calls structured dashboard logic
Tool	/tool/generate_rag_dashboard	Calls RAG dashboard logic
Resource	/resource/ai50/companies	Lists company IDs
Prompt	/prompt/pe-dashboard	Returns 8-section dashboard template

Provide Dockerfile (Dockerfile.mcp) and .env variables for config.

‚úÖ Checkpoint: MCP Inspector shows registered tools/resources/prompts.

‚∏ª

Lab 15 ‚Äî Agent MCP Consumption
	‚Ä¢	Configure mcp_config.json with base URL and tools.
	‚Ä¢	Allow Supervisor Agent to invoke MCP tools securely with tool filtering.
	‚Ä¢	Add integration test (tests/test_mcp_server.py) that requests a dashboard.

‚úÖ Checkpoint: Agent ‚Üí MCP ‚Üí Dashboard ‚Üí Agent round trip works.

‚∏ª

üß† Phase 3 ‚Äì Advanced Agent Implementation (Labs 16‚Äì18)

Lab 16 ‚Äî ReAct Pattern Implementation
	‚Ä¢	Log Thought/Action/Observation triplets in structured JSON (log file or stdout).
	‚Ä¢	Use correlation IDs (run_id, company_id).
	‚Ä¢	Save one trace under docs/REACT_TRACE_EXAMPLE.md.

‚úÖ Checkpoint: JSON logs show sequential ReAct steps.

‚∏ª

Lab 17 ‚Äî Supervisory Workflow Pattern (Graph-based)

Use LangGraph or WorkflowBuilder to define nodes:

Node	Responsibility
Planner	Constructs plan of actions
Data Generator	Invokes MCP dashboard tools
Evaluator	Scores dashboards per rubric
Risk Detector	Branches to HITL if keywords found

Provide workflow diagram (docs/WORKFLOW_GRAPH.md) and unit test covering both branches.

‚úÖ Checkpoint: python src/workflows/due_diligence_graph.py prints branch taken.

‚∏ª

Lab 18 ‚Äî HITL Integration & Visualization
	‚Ä¢	Implement CLI or HTTP pause for human approval.
	‚Ä¢	Record execution path with LangGraph Dev UI or Mermaid.
	‚Ä¢	Save trace and decision path in docs/REACT_TRACE_EXAMPLE.md.

‚úÖ Checkpoint: Demo video shows workflow pausing and resuming after approval.

‚∏ª

‚òÅÔ∏è Phase 4 ‚Äì Orchestration & Deployment (Add-On)

Airflow DAGs Integration

Create under airflow/dags/:

File	Purpose
orbit_initial_load_dag.py	Initial data load and payload assembly
orbit_daily_update_dag.py	Incremental updates of snapshots and vector DB
orbit_agentic_dashboard_dag.py	Invokes MCP + Agentic workflow daily for all AI 50 companies

‚úÖ Checkpoint: Each DAG runs locally or in Dockerized Airflow and updates dashboards.

Containerization and Configuration

Provide:
	‚Ä¢	Dockerfile.mcp (for MCP Server)
	‚Ä¢	Dockerfile.agent (for Supervisor Agent + Workflow)
	‚Ä¢	docker-compose.yml linking services + optional vector DB
	‚Ä¢	.env.example for API keys and service URLs
	‚Ä¢	config/settings_example.yaml for parameterization

‚úÖ Checkpoint: docker compose up brings up MCP + Agent locally.

‚∏ª

üß™ Testing & Observability

Minimum Tests (pytest)

Test	Purpose
test_tools.py	Validate core tools return expected schema
test_mcp_server.py	Ensure MCP endpoints return Markdown
test_workflow_branches.py	Assert risk vs no-risk branch logic

Run: pytest -v --maxfail=1 --disable-warnings

Logging & Metrics
	‚Ä¢	Use Python logging or structlog (JSON format).
	‚Ä¢	Include fields: timestamp, run_id, company_id, phase, message.
	‚Ä¢	Optional: emit basic counters (e.g., dashboards generated, HITL triggered).

‚∏ª

üì¶ Deliverables

#	Deliverable	Requirements
1	Updated GitHub Repo (pe-dashboard-ai50-v3)	Full code + docs + Airflow DAGs
2	MCP Server Service	Dockerized HTTP server exposing Tools/Resources/Prompts
3	Supervisor Agent & Workflow	Implements ReAct + Graph + HITL
4	Airflow Integration	DAG invokes Agentic workflow on schedule
5	Configuration Mgmt	.env and config/ externalization
6	Testing Suite	‚â• 3 pytest cases
7	Structured Logging	JSON ReAct trace saved to docs/
8	Docker Deployment	Dockerfiles + docker-compose
9	Demo Video (‚â§ 5 min)	Show workflow execution + HITL pause
10	Contribution Attestation	Completed form


‚∏ª

üßÆ Dashboard Format (Reference)

Eight mandatory sections:
	1.	Company Overview
	2.	Business Model and GTM
	3.	Funding & Investor Profile
	4.	Growth Momentum
	5.	Visibility & Market Sentiment
	6.	Risks and Challenges
	7.	Outlook
	8.	Disclosure Gaps (bullet list of missing info)

Rules
	‚Ä¢	Use literal ‚ÄúNot disclosed.‚Äù for missing fields.
	‚Ä¢	Never invent ARR/MRR/valuation/customer logos.
	‚Ä¢	Always include final Disclosure Gaps section.

‚∏ª

üöÄ Production Readiness Checklist

Before submission, verify that your system:
	‚Ä¢	Has working Airflow DAGs for initial/daily/agentic runs
	‚Ä¢	Runs MCP Server + Agent via Docker Compose
	‚Ä¢	Loads config and secrets from .env or config/
	‚Ä¢	Implements structured ReAct logging (JSON)
	‚Ä¢	Includes at least 3 automated pytest tests
	‚Ä¢	Documents setup and run instructions in README.md
	‚Ä¢	Demo video shows HITL pause/resume
	‚Ä¢	README contains system diagram and architecture summary

‚∏ª

üßæ Submission
	‚Ä¢	Repo name: pe-dashboard-ai50-v3-<teamname>
	‚Ä¢	Push to GitHub with all code, docs, and Docker/Airflow files.
	‚Ä¢	Include demo video link in README.
	‚Ä¢	Submit GitHub URL + video link via LMS.

‚∏ª

üìö References & Resources
	‚Ä¢	Python AI Series modules (Structured Outputs, Tool Calling, Agents, MCP)
	‚Ä¢	Model Context Protocol Docs
	‚Ä¢	LangGraph Docs
	‚Ä¢	Microsoft Agent Framework Samples
	‚Ä¢	Apache Airflow Quick Start
	‚Ä¢	Docker Compose Guide

---

## Contributions

- Swara: Phase 1 (Labs 0‚Äì1), Phase 2 (Lab 5), Phase 3 (Labs 8‚Äì9)
- Nat: Phase 1 (Labs 2‚Äì3), Phase 4 (Labs 10‚Äì11)
- Kundana: Phase 2 (Labs 4, 6),Phase 3(Lab 7)

### Roles & Responsibilities

- Swara
  - Phase 1, 3

- Nat
  - Phase 4
  - RAG pipeline

- Kundana
  - Phase 2,3
